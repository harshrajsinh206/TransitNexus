# -*- coding: utf-8 -*-
"""P1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1DIf8cuLS8aFDi-7O8IBgiuA5xHg2ApqA
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.model_selection import train_test_split, GridSearchCV
from sklearn.preprocessing import StandardScaler, LabelEncoder
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from lightgbm import LGBMRegressor

from google.colab import drive

# Mount Google Drive
drive.mount('/content/drive')

file_path = '/content/drive/MyDrive/MTA/MTA_Subway_Hourly_Ridership__Beginning_February_2022.csv'
df = pd.read_csv(file_path)
df

q1 = df["ridership"].quantile(0.25)
q3 = df["ridership"].quantile(0.75)
iqr = q3 - q1
lower_bound = q1 - 1.5 * iqr
upper_bound = q3 + 1.5 * iqr
df = df[(df["ridership"] >= lower_bound) & (df["ridership"] <= upper_bound)]

print("Dataset after removing outliers:", df.shape)

datetime_column = None
for col in df.columns:
    if "date" in col.lower() or "time" in col.lower():  # Find a similar column
        datetime_column = col
        break

if datetime_column:
    df[datetime_column] = pd.to_datetime(df[datetime_column])

    df["hour"] = df[datetime_column].dt.hour
    df["day"] = df[datetime_column].dt.day
    df["month"] = df[datetime_column].dt.month
    df["weekday"] = df[datetime_column].dt.weekday

    #New features
    df["is_weekend"] = df["weekday"].apply(lambda x: 1 if x >= 5 else 0)
    df["rush_hour"] = df["hour"].apply(lambda x: 1 if (7 <= x <= 9 or 17 <= x <= 19) else 0)

    # Drop original datetime column
    df.drop(columns=[datetime_column], inplace=True)

    print(f"Feature engineering done using column: {datetime_column}")
else:
    print("No datetime column found in the dataset!")

# Select categorical features
categorical_cols = ["borough", "payment_method", "fare_class_category"]

# Encode categorical features if they exist in the dataset
encoder = LabelEncoder()
for col in categorical_cols:
    if col in df.columns:
        df[col] = encoder.fit_transform(df[col])

print(df.dtypes)  # Show column types

from sklearn.preprocessing import LabelEncoder

# Identify categorical columns (non-numeric)
categorical_cols = df.select_dtypes(include=['object']).columns.tolist()

# Encode categorical columns
encoder = LabelEncoder()
for col in categorical_cols:
    df[col] = encoder.fit_transform(df[col])

# updated column types
print(df.dtypes)

# target variable
target_column = "ridership"  # Change this if needed

# Drop unnecessary columns
X = df.drop(columns=[target_column], axis=1)
y = df[target_column]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Normalize numerical features
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

print("✅ Data is ready for model training!")

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# drop the target variable
target_column = "ridership"
X = df.drop(columns=[target_column], axis=1)
y = df[target_column]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# X_train and X_test have the same columns
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)  # Align columns

# StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data
X_test_scaled = scaler.transform(X_test)  # Transform test data

print(f"✅ Data preprocessing completed. Feature count: {X_train.shape[1]}")

import datetime
import numpy as np
import pandas as pd
from sklearn.preprocessing import LabelEncoder

# Function to assign season based on the month
def get_season(month):
    if month in [12, 1, 2]:
        return "Winter"
    elif month in [3, 4, 5]:
        return "Spring"
    elif month in [6, 7, 8]:
        return "Summer"
    else:
        return "Fall"

# Function to check if the date is a public holiday
holidays = {
    (1, 1), (7, 4), (12, 25)  # New Year, Independence Day, Christmas
}

def is_holiday(day, month):
    return 1 if (month, day) in holidays else 0

# Adding new features
df["season"] = df["month"].apply(get_season)
df["holiday"] = df.apply(lambda x: is_holiday(x["day"], x["month"]), axis=1)

# Encode categorical season feature
df["season"] = LabelEncoder().fit_transform(df["season"])

# Add special event feature (for now, assume some random dates)
df["special_event"] = np.random.choice([0, 1], size=len(df), p=[0.9, 0.1])  # 10% chance of an event

# Shift ridership to create a previous hour feature
df["previous_hour_ridership"] = df["ridership"].shift(1).fillna(df["ridership"].mean())

# Print confirmation
print("✅ New features added:", ["season", "holiday", "special_event", "previous_hour_ridership"])

# Frequency encoding for categorical variables
categorical_cols = ["borough", "fare_class_category"]

for col in categorical_cols:
    if col in df.columns:
        df[col + "_freq"] = df[col].map(df[col].value_counts(normalize=True))
        df.drop(columns=[col], inplace=True)  # Drop original column

print("✅ Applied frequency encoding to categorical columns.")

# Define model
lgb_model = LGBMRegressor()

# Define hyperparameter grid
param_grid = {
    "n_estimators": [200, 500],
    "learning_rate": [0.01, 0.05, 0.1],
    "max_depth": [5, 10, 15],
    "num_leaves": [20, 50, 100]
}
# Grid Search
grid_search = GridSearchCV(lgb_model, param_grid, scoring="r2", cv=3, verbose=2, n_jobs=-1)
grid_search.fit(X_train, y_train)


# Best model
best_model = grid_search.best_estimator_f
print("Best Parameters:", grid_search.best_params_)

# Predictions
y_pred = best_model.predict(X_test)

# Performance metrics
mae = mean_absolute_error(y_test, y_pred)
rmse = np.sqrt(mean_squared_error(y_test, y_pred))
r2 = r2_score(y_test, y_pred)

print(f"Final Model MAE: {mae}")
print(f"Final Model RMSE: {rmse}")
print(f"Final Model R² Score: {r2}")

plt.figure(figsize=(12, 6))
plt.scatter(df["datetime"], df["ridership"], label="Existing Data", color="blue", alpha=0.5)
plt.scatter(future_dates, future_ridership, label="Predicted Ridership", color="red", marker="x")
plt.xlabel("Date")
plt.ylabel(f"Ridership ({unit})")
plt.title("Existing and Predicted Ridership Trends (Scatter Plot)")
plt.legend()
plt.grid(True)
plt.show()

plt.figure(figsize=(12, 6))
plt.bar(df["datetime"], df["ridership"], label="Existing Data", color="blue", alpha=0.6)
plt.bar(future_dates, future_ridership, label="Predicted Ridership", color="red", alpha=0.6)
plt.xlabel("Date")
plt.ylabel(f"Ridership ({unit})")
plt.title("Existing and Predicted Ridership Trends (Bar Chart)")
plt.legend()
plt.xticks(rotation=45)
plt.show()

from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split

# Define target variable
target_column = "ridership"

# Ensure we drop the target variable
X = df.drop(columns=[target_column], axis=1)
y = df[target_column]

# Train-test split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Ensure X_train and X_test have the same columns
X_test = X_test.reindex(columns=X_train.columns, fill_value=0)  # Align columns

# Apply StandardScaler
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)  # Fit and transform training data
X_test_scaled = scaler.transform(X_test)  # Transform test data

print(f"✅ Data preprocessing completed. Feature count: {X_train.shape[1]}")

print("Missing columns in X_test:", set(X_train.columns) - set(X_test.columns))
print("Extra columns in X_test:", set(X_test.columns) - set(X_train.columns))

print(df.columns)





